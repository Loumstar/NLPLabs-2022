{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "military-character",
   "metadata": {
    "id": "military-character"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EslAmsE-tWMB",
   "metadata": {
    "id": "EslAmsE-tWMB"
   },
   "source": [
    "In this lab, we will take you through a practical use of Transformers. This notebook shows you how to use [Hugging face](https://huggingface.co/)'s package to import and train pretrained models for the tasks of hate speech classification and machine translation.\n",
    "\n",
    "We first show you all necessay components to use the ``transformers`` package before asking you to implement some code in the later sections.\n",
    "\n",
    "\n",
    "**Note:** The training of models will take quite some time so make sure to run this session with the GPU enabled. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rwXjbNUJHzZ0",
   "metadata": {
    "id": "rwXjbNUJHzZ0"
   },
   "source": [
    "## Setting up the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vina6dhaHz1N",
   "metadata": {
    "id": "Vina6dhaHz1N"
   },
   "source": [
    "First, we need to install Hugging Face [transformers](https://huggingface.co/transformers/index.html) and [Sentence piece Tokenizers](https://github.com/google/sentencepiece) with the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "other-scottish",
   "metadata": {
    "id": "other-scottish"
   },
   "outputs": [],
   "source": [
    "#! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "modern-olympus",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "modern-olympus",
    "outputId": "629d3a85-7a90-48b6-e132-bc979c12a741"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.17.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.8/site-packages (0.1.96)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (7.7.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (6.9.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (8.0.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.3)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (1.1.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.6.0)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.4)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.1.2)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (59.5.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: black in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (22.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.26)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (2.11.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (4.9.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (4.4.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (5.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (18.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.7.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.8/site-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.4.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.13.1)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.3.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.9.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.3)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.4.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in /opt/conda/lib/python3.8/site-packages (from black->ipython>=4.0.0->ipywidgets) (4.0.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.8/site-packages (from black->ipython>=4.0.0->ipywidgets) (8.0.3)\n",
      "Requirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.8/site-packages (from black->ipython>=4.0.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from black->ipython>=4.0.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /opt/conda/lib/python3.8/site-packages (from black->ipython>=4.0.0->ipywidgets) (0.9.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /opt/conda/lib/python3.8/site-packages (from black->ipython>=4.0.0->ipywidgets) (0.4.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.11)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.7)\n",
      "Requirement already satisfied: asttokens in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: executing in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "developing-france",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "developing-france",
    "outputId": "bbc8a705-01c2-4b00-8081-1b619c887da1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "  print('WARNING: You may want to change the runtime to GPU for faster training!')\n",
    "  DEVICE = 'cpu'\n",
    "else:\n",
    "  DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58jPlYaWk7fh",
   "metadata": {
    "id": "58jPlYaWk7fh"
   },
   "source": [
    "If you work in Colab, mount your google drive to save models and training checkpoints. Run the following code to connect your google drive to colab. Click on the link and copy and past the code you saw into the input box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qJ-jH0JoVG1v",
   "metadata": {
    "id": "qJ-jH0JoVG1v"
   },
   "source": [
    "# Hate Speech Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-syndicate",
   "metadata": {
    "id": "cognitive-syndicate"
   },
   "source": [
    "## Downloading the dataset\n",
    "\n",
    "For the task of hate speech classification, we will work with the [Offensive Language Identification Dataset - OLID ](https://scholar.harvard.edu/malmasi/olid). It is a dataset of tweets hierarchically annotated on three levels: \n",
    "\n",
    "* Level A: Offensive Language Detection\n",
    "* Level B: Categorization of Offensive Language\n",
    "* Level C: Offensive Language Target Identification\n",
    "\n",
    "\n",
    "Let's download it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eISPHZ-7HwBk",
   "metadata": {
    "id": "eISPHZ-7HwBk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/louismanestar/Library/CloudStorage/OneDrive-ImperialCollegeLondon/MSc Artificial Intelligence/Tutorials & Coursework/Natural Language Processing/Labs/lab05-BERT-for-hate-speech-detection/data\n",
      "zsh:1: no matches found: https://www.dropbox.com/s/bavjtyx0ndty7xt/pretrain.txt?dl=0\n",
      "zsh:1: no matches found: https://sites.google.com/site/offensevalsharedtask/olid/OLIDv1.0.zip?attredirects=0\n",
      "unzip:  cannot find or open OLIDv1.0.zip, OLIDv1.0.zip.zip or OLIDv1.0.zip.ZIP.\n",
      "/Users/louismanestar/Library/CloudStorage/OneDrive-ImperialCollegeLondon/MSc Artificial Intelligence/Tutorials & Coursework/Natural Language Processing/Labs/lab05-BERT-for-hate-speech-detection\n"
     ]
    }
   ],
   "source": [
    "%mkdir ./data\n",
    "%cd ./data\n",
    "\n",
    "if not os.path.isfile('pretrain.txt'): \n",
    "    !wget -O pretrain.txt https://www.dropbox.com/s/bavjtyx0ndty7xt/pretrain.txt?dl=0\n",
    "\n",
    "if not os.path.isfile('OLIDv1.0.zip'): \n",
    "    !wget -O OLIDv1.0.zip https://sites.google.com/site/offensevalsharedtask/olid/OLIDv1.0.zip?attredirects=0&d=1\n",
    "    !unzip OLIDv1.0.zip\n",
    "  \n",
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TsIt3OVmNnRY",
   "metadata": {
    "id": "TsIt3OVmNnRY"
   },
   "source": [
    "Let's have a look at the data we downloaded.\n",
    "\n",
    "As mentioned above, the ``OLID`` dataset has been labeled for three subtask, therefore we have three different labels sets per tweet: \n",
    "* Task A: Not Offensive (``NOT``) and Offensive (``OFF``).\n",
    "* Task B: Targeted Insult (``TIN``), Untargeted (``UNT``) and ``NULL`` for not offensive tweets.\n",
    "* Task C: Individual (``IND``), Group (``GRP``), Other (``OTH``) and ``NULL`` for not offensive and non targeted tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rFwKBesVNmmr",
   "metadata": {
    "id": "rFwKBesVNmmr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 13240\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT       NaN  \n",
       "1       TIN       IND  \n",
       "2       NaN       NaN  \n",
       "3       UNT       NaN  \n",
       "4       NaN       NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/olid-training-v1.0.tsv',delimiter=\"\\t\")\n",
    "\n",
    "print(f'Number of training samples: {len(df)}')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-southeast",
   "metadata": {
    "id": "published-southeast"
   },
   "source": [
    "## Loading and preprocessing the corpus \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nw6m7B6rEL6E",
   "metadata": {
    "id": "nw6m7B6rEL6E"
   },
   "source": [
    "Let's define ``reader_train`` and ``reader_test`` that will prepare our data corpus and labels for both train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "correct-tuner",
   "metadata": {
    "id": "correct-tuner"
   },
   "outputs": [],
   "source": [
    "def reader_train(file_name):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    fin = open(file_name)\n",
    "    title = fin.readline()\n",
    "    set_a = ['NOT' , 'OFF']\n",
    "    set_b = ['NULL', 'TIN', 'UNT']\n",
    "    set_c = ['NULL', 'IND', 'GRP', 'OTH']\n",
    "    while True:\n",
    "        line = fin.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        items = line.split('\\t')\n",
    "        text = items[1]\n",
    "        label_a = set_a.index(items[2].strip())\n",
    "        label_b = set_b.index(items[3].strip())\n",
    "        label_c = set_c.index(items[4].strip())\n",
    "\n",
    "        if len(text) > 0:\n",
    "            texts.append(text)\n",
    "            labels.append([label_a, label_b, label_c])\n",
    "            \n",
    "    return {'texts':texts, 'labels':labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "designed-screening",
   "metadata": {
    "id": "designed-screening"
   },
   "outputs": [],
   "source": [
    "def reader_test(test_textlist, test_labellist):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    text_dict = {}\n",
    "    \n",
    "    # build text_dict\n",
    "    for file_text in test_textlist:\n",
    "        fin = open(file_text)\n",
    "        title = fin.readline()\n",
    "        while True:\n",
    "            line = fin.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            items = line.split('\\t')\n",
    "            if items[0] not in text_dict:\n",
    "                text_dict[items[0]] = items[1]\n",
    "        fin.close()\n",
    "    label_dict_list = []\n",
    "    \n",
    "    # build label_dict\n",
    "    for i, file_label in enumerate(test_labellist):\n",
    "        label_dict_list.append({})\n",
    "        fin = open(file_label)\n",
    "        title = fin.readline()\n",
    "        while True:\n",
    "            line = fin.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            items = line.split(',')\n",
    "            label_dict_list[i][items[0]] = items[1]\n",
    "        fin.close()    \n",
    "    \n",
    "    set_a = ['NOT' , 'OFF']\n",
    "    set_b = ['NULL', 'TIN', 'UNT']\n",
    "    set_c = ['NULL', 'IND', 'GRP', 'OTH']\n",
    "    \n",
    "    for idx, text in text_dict.items():\n",
    "        if len(text) > 0:\n",
    "            texts.append(text)\n",
    "            if idx in label_dict_list[0]:\n",
    "                label_a = label_dict_list[0][idx]\n",
    "            else:\n",
    "                label_a = 'OFF'\n",
    "            if idx in label_dict_list[1]:\n",
    "                label_b = label_dict_list[1][idx]\n",
    "            else:\n",
    "                label_b = 'NULL'\n",
    "            if idx in label_dict_list[2]:\n",
    "                label_c = label_dict_list[2][idx]\n",
    "            else:\n",
    "                label_c = 'NULL'\n",
    "            \n",
    "            label_a = set_a.index(label_a.strip())\n",
    "            label_b = set_b.index(label_b.strip())\n",
    "            label_c = set_c.index(label_c.strip())\n",
    "        \n",
    "            labels.append([label_a, label_b, label_c])\n",
    "            \n",
    "    return {'texts':texts, 'labels':labels}            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T4kLz8nca7ix",
   "metadata": {
    "id": "T4kLz8nca7ix"
   },
   "source": [
    "We also define our custom ``OlidDataset`` class which allows us to control how we handle the iteration and batches.\n",
    "\n",
    "At each iteration over the dataset object, the function ``__get_item__`` is called and returns a list of dictionnaries with the tweets and their 3 labels. \n",
    "Then, the ``collate_fn`` function will process the list of samples into their encodings and return a batch when called by the iterator during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "amber-exposure",
   "metadata": {
    "id": "amber-exposure"
   },
   "outputs": [],
   "source": [
    "class OlidDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer, input_set):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = input_set['texts']\n",
    "        self.labels = input_set['labels']\n",
    "        \n",
    "    def collate_fn(self, batch):\n",
    "\n",
    "        texts = []\n",
    "        labels_a = []\n",
    "        labels_b = []\n",
    "        labels_c = []\n",
    "\n",
    "        for b in batch:\n",
    "            texts.append(b['text'])\n",
    "            labels_a.append(b['label_a'])\n",
    "            labels_b.append(b['label_b'])\n",
    "            labels_c.append(b['label_c'])\n",
    "\n",
    "        #The maximum sequence size for BERT is 512 but here the tokenizer truncate sentences longer than 128 tokens.  \n",
    "        # We also pad shorter sentences to a length of 128 tokens\n",
    "        encodings = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        labels = {}\n",
    "        encodings['label_a'] =  torch.tensor(labels_a)\n",
    "        encodings['label_b'] =  torch.tensor(labels_b)\n",
    "        encodings['label_c'] =  torch.tensor(labels_c)\n",
    "        \n",
    "        return encodings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "       \n",
    "        item = {'text': self.texts[idx],\n",
    "                'label_a': self.labels[idx][0],\n",
    "                'label_b': self.labels[idx][1],\n",
    "                'label_c': self.labels[idx][2]}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66CvfwkHbXO_",
   "metadata": {
    "id": "66CvfwkHbXO_"
   },
   "source": [
    "\n",
    "Now let's put it all together and load our data. Here we use a pre-made tokenizer that was used for our BERT model. Here we pick the pre-trained model ``bert-base-cased``. There are several other models of various sizes (base, large).\n",
    "\n",
    "**Note:** ``bert-base-cased`` is case-sensitive and it differenciates English from english. An non case-sensitive variant is ``bert-base-uncased``.\n",
    "\n",
    "You can always use another [tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html), but we will get better results using the same tokenizer as the one used to pre-train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nHzy19X9TUqc",
   "metadata": {
    "id": "nHzy19X9TUqc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='bert-base-cased', vocab_size=28996, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# we can check the parameters of this tokenizer\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deluxe-biography",
   "metadata": {
    "id": "deluxe-biography"
   },
   "outputs": [],
   "source": [
    "trainset = reader_train('./data/olid-training-v1.0.tsv')\n",
    "testset = reader_test(['./data/testset-levela.tsv','./data/testset-levelb.tsv','./data/testset-levelc.tsv'], \n",
    "                      ['./data/labels-levela.csv','./data/labels-levelb.csv','./data/labels-levelc.csv'])\n",
    "\n",
    "train_dataset = OlidDataset(tokenizer, trainset)\n",
    "test_dataset = OlidDataset(tokenizer, testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oxEXetnGKVj2",
   "metadata": {
    "id": "oxEXetnGKVj2"
   },
   "source": [
    "The following code let's you play around with our ``train_dataset`` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1Q2VvuoaKFXB",
   "metadata": {
    "id": "1Q2VvuoaKFXB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[101, 137, 1646, 9637, 1153, 1431, 2367, 170, 1374, 2900, 4038, 1184, 1147, 1321, 1113, 1142, 1110, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 137, 1646, 9637, 137, 1646, 9637, 3414, 1313, 1128, 787, 1231, 6882, 106, 106, 106, 137, 1646, 9637, 108, 9960, 10583, 108, 8499, 10973, 10973, 100, 158, 20550, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 9786, 1110, 11950, 1922, 4570, 1150, 1132, 4147, 4422, 2233, 1106, 1503, 118, 1710, 18275, 1116, 1702, 1111, 1126, 2652, 1107, 1103, 6591, 24210, 119, 158, 20550, 108, 9786, 108, 9960, 10583, 108, 148, 22689, 108, 24890, 11607, 1592, 108, 157, 15678, 1942, 102, 0, 0, 0], [101, 107, 137, 1646, 9637, 6518, 1431, 112, 1396, 1942, 9899, 1179, 107, 107, 1142, 2727, 1104, 4170, 1106, 170, 15406, 119, 100, 107, 107, 107, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 137, 1646, 9637, 137, 1646, 9637, 7661, 1458, 7691, 1116, 111, 1821, 1643, 132, 5696, 1116, 1106, 1815, 1154, 1894, 2231, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 137, 1646, 9637, 16951, 1132, 1155, 19892, 26793, 1186, 106, 106, 106, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 137, 1646, 9637, 137, 1646, 9637, 2048, 1185, 1279, 106, 1706, 6289, 4170, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 137, 1646, 9637, 1108, 6290, 1198, 2520, 1164, 1142, 25338, 1233, 1155, 3367, 4598, 1116, 1176, 1115, 1138, 1151, 1383, 12534, 119, 1122, 787, 188, 11516, 1215, 1106, 13330, 1366, 1113, 1558, 2492, 1176, 2560, 1654, 1105, 12010, 102, 0, 0, 0, 0, 0, 0, 0, 0], [101, 137, 1646, 9637, 26123, 1167, 2854, 13782, 2312, 106, 106, 106, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 137, 1646, 9637, 1803, 2144, 787, 189, 1444, 1330, 140, 21986, 2428, 106, 1284, 1640, 1138, 1536, 108, 10605, 18066, 2162, 11470, 1204, 108, 16951, 175, 115, 115, 2226, 1146, 1412, 1632, 1583, 106, 108, 154, 20324, 1116, 108, 157, 25980, 8221, 2107, 8954, 2349, 1186, 102]]\n",
      "token_type_ids: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "label_a: [1, 1, 0, 1, 0, 1, 1, 1, 0, 1]\n",
      "label_b: [2, 1, 0, 2, 0, 1, 2, 1, 0, 1]\n",
      "label_c: [0, 1, 0, 0, 0, 3, 0, 2, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "#returns first item as dictionnary\n",
    "#print(train_dataset[0])\n",
    "\n",
    "# put all train set into one batch for the collate_fn function\n",
    "batch = [sample for sample in train_dataset]\n",
    "\n",
    "encodings = train_dataset.collate_fn(batch[:10])\n",
    "\n",
    "for key, value in encodings.items():\n",
    "  print(f\"{key}: {value.numpy().tolist()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-closure",
   "metadata": {
    "id": "opponent-closure"
   },
   "source": [
    "## Finetuning a pre-trained BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dpaMPoB5cNwt",
   "metadata": {
    "id": "dpaMPoB5cNwt"
   },
   "source": [
    "\n",
    "As you can recall from the lecture, BERT is a model trained on Masked language Modeling(MLM) and Next Sentence Prediction(NSP), however is not trained to do to do sentence classification. We then need to adapt it for hate speech classification and finetune the pre-trained model on our dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n2zPF-H3nHDH",
   "metadata": {
    "id": "n2zPF-H3nHDH"
   },
   "source": [
    "Let's have a look at ``bert_base-uncased`` summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pk_QEpoGnZhL",
   "metadata": {
    "id": "pk_QEpoGnZhL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 108310272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "#180 M\n",
    "print(f\"Model size: {model.num_parameters()}\")\n",
    "\n",
    "#model summary\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GBOHvhd-n00L",
   "metadata": {
    "id": "GBOHvhd-n00L"
   },
   "source": [
    "Note that the model has only encoder layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ESBsGVb_e2QT",
   "metadata": {
    "id": "ESBsGVb_e2QT"
   },
   "source": [
    "### BERT Model\n",
    "\n",
    "To define our model, we will build on top of a Huggingface pre-trained model and adapt it to our task. We will use ``BertModel`` to extract embeddings and add a ``Linear`` layer to classify samples. Hugging face implementation of BERT can handle different variations of the model, which we define and pass its parameter values via``config``.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb9AN0YFWDe",
   "metadata": {
    "id": "feb9AN0YFWDe"
   },
   "source": [
    "The code below defines a model adapted to classify tweets on Level A, Offensive Language Detection. We will implement Task B and C later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bottom-tribute",
   "metadata": {
    "id": "bottom-tribute"
   },
   "outputs": [],
   "source": [
    "class BERT_hate_speech(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # BERT Model\n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        # Task A\n",
    "        self.projection_a = torch.nn.Sequential(torch.nn.Dropout(0.2),\n",
    "                                                torch.nn.Linear(config.hidden_size, 2))\n",
    "      \n",
    "        # the sigmoid activation here is not strictly necessary.\n",
    "        # However, its presence here doesn't make a big difference to the model's performance\n",
    "        # Why do you think that is?\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "\n",
    "        # Task B\n",
    "        # TBA\n",
    "        \n",
    "        # Task C\n",
    "        # TBA\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None):\n",
    " \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # Logits A\n",
    "        logits_a = self.projection_a(outputs[1])\n",
    "        out =  self.activation(logits_a)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yMyL5gGKfECs",
   "metadata": {
    "id": "yMyL5gGKfECs"
   },
   "source": [
    "### Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QaKWwqYMlCVX",
   "metadata": {
    "id": "QaKWwqYMlCVX"
   },
   "source": [
    "Finally, we should define our training loop. Fortunately, the ``transformers`` package provides us with a [``Trainer``](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer) class wich takes care of the training of transformers models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MjljHz4nlAbJ",
   "metadata": {
    "id": "MjljHz4nlAbJ"
   },
   "source": [
    "We build our custom ``Trainer`` class to incorporate our own ``compute_loss`` function over the three labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "engaged-perspective",
   "metadata": {
    "id": "engaged-perspective"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Trainer_hate_speech(Trainer):\n",
    "    def compute_loss(self, model, inputs):\n",
    "        labels = {}\n",
    "        labels['label_a'] = inputs.pop('label_a')\n",
    "        labels['label_b'] = inputs.pop('label_b')\n",
    "        labels['label_c'] = inputs.pop('label_c')\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # TASK A\n",
    "        loss_task_a = nn.CrossEntropyLoss()\n",
    "        labels_a = labels['label_a']\n",
    "        loss_a = loss_task_a(outputs.view(-1, 2), labels_a.view(-1))\n",
    "\n",
    "        loss = loss_a\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0WDX456F4YB",
   "metadata": {
    "id": "p0WDX456F4YB"
   },
   "source": [
    "\n",
    "Now let's finetune the pretrained model on our ``OlidDataset``.\n",
    "\n",
    "In our function ``main_hate_speech``we define the arguments for the ``Trainer`` object and launch the training with ``trainer.train``. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "killing-population",
   "metadata": {
    "id": "killing-population"
   },
   "outputs": [],
   "source": [
    "def main_hate_speech():\n",
    "\n",
    "    #call our custom BERT model and pass as parameter the name of an available pretrained model\n",
    "    model = BERT_hate_speech.from_pretrained(\"bert-base-cased\")\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./experiment/hate_speech',\n",
    "        learning_rate = 0.0001,\n",
    "        logging_steps= 100,\n",
    "        per_device_train_batch_size=32,\n",
    "        num_train_epochs = 3,\n",
    "    )\n",
    "    trainer = Trainer_hate_speech(\n",
    "        model=model,                         \n",
    "        args=training_args,                 \n",
    "        train_dataset=train_dataset,                   \n",
    "        data_collator=train_dataset.collate_fn\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.save_model('./models/ht_bert_finetuned/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OanfT22IduY6",
   "metadata": {
    "id": "OanfT22IduY6"
   },
   "source": [
    "Let's run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "looking-escape",
   "metadata": {
    "id": "looking-escape"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BERT_hate_speech: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BERT_hate_speech from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BERT_hate_speech from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BERT_hate_speech were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['projection_a.1.bias', 'projection_a.1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 13240\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1242\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1242' max='1242' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1242/1242 08:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.604400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.562400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.547800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.548100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.607600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.647000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.630100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.636800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.618500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.566600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.528800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.535000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./experiment/hate_speech/checkpoint-500\n",
      "Configuration saved in ./experiment/hate_speech/checkpoint-500/config.json\n",
      "Model weights saved in ./experiment/hate_speech/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./experiment/hate_speech/checkpoint-1000\n",
      "Configuration saved in ./experiment/hate_speech/checkpoint-1000/config.json\n",
      "Model weights saved in ./experiment/hate_speech/checkpoint-1000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./models/ht_bert_finetuned/\n",
      "Configuration saved in ./models/ht_bert_finetuned/config.json\n",
      "Model weights saved in ./models/ht_bert_finetuned/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "main_hate_speech()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6Q0sYQeoM90U",
   "metadata": {
    "id": "6Q0sYQeoM90U"
   },
   "source": [
    "### Evaluation\n",
    "Once we trained our model, we can evaluate it on our test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tt8KRJs2OSo1",
   "metadata": {
    "id": "tt8KRJs2OSo1"
   },
   "source": [
    "Let's define a helper function ``predict_hatespeech`` that will extract the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "leJGKWvXGDf7",
   "metadata": {
    "id": "leJGKWvXGDf7"
   },
   "outputs": [],
   "source": [
    "def predict_hatespeech(input, tokenizer, model): \n",
    "  model.eval()\n",
    "  encodings = tokenizer(input, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "  \n",
    "  output = model(**encodings)\n",
    "  preds = torch.max(output, 1)\n",
    "\n",
    "  return {'prediction':preds[1], 'confidence':preds[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_y2mgB3-PQpI",
   "metadata": {
    "id": "_y2mgB3-PQpI"
   },
   "source": [
    "Now let's define a function that will evaluate our model on the test set we prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2olq7egUHYnH",
   "metadata": {
    "id": "2olq7egUHYnH"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, data_loader):\n",
    "\n",
    "  total_count = 0\n",
    "  correct_count = 0 \n",
    "\n",
    "  preds = []\n",
    "  tot_labels = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for data in tqdm(data_loader): \n",
    "\n",
    "      labels = {}\n",
    "      labels['label_a'] = data['label_a']\n",
    "\n",
    "      tweets = data['text']\n",
    "\n",
    "      pred = predict_hatespeech(tweets, tokenizer, model)\n",
    "\n",
    "      preds.append(pred['prediction'])\n",
    "      tot_labels.append(labels['label_a'])\n",
    "\n",
    "  #with the saved predictions and labels we can compute accuracy, precision, recall and f1-score\n",
    "  report = classification_report(tot_labels, preds, target_names=[\"Not offensive\",\"Offensive\"], output_dict= True)\n",
    "\n",
    "  return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "Sps-kXvWeNoO",
   "metadata": {
    "id": "Sps-kXvWeNoO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file ./models/ht_bert_finetuned/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BERT_hate_speech\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file ./models/ht_bert_finetuned/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BERT_hate_speech.\n",
      "\n",
      "All the weights of BERT_hate_speech were initialized from the model checkpoint at ./models/ht_bert_finetuned/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BERT_hate_speech for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feeb77f332ef43ee9fefad8ee8f1efdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/860 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:147: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  y = np.asarray(y)\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:151: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  y = np.array(y, dtype=object)\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:269: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  y = np.asarray(y)\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:273: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  y = np.asarray(y, dtype=object)\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:147: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  y = np.asarray(y)\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:151: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  y = np.array(y, dtype=object)\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:269: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  y = np.asarray(y)\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:273: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  y = np.asarray(y, dtype=object)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unknown is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# we don't batch our test set unless it's too big\u001b[39;00m\n\u001b[1;32m      8\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset)\n\u001b[0;32m---> 10\u001b[0m report \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(report)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(report[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, tokenizer, data_loader)\u001b[0m\n\u001b[1;32m     20\u001b[0m     tot_labels\u001b[38;5;241m.\u001b[39mappend(labels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_a\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#with the saved predictions and labels we can compute accuracy, precision, recall and f1-score\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m report \u001b[38;5;241m=\u001b[39m \u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtot_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNot offensive\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOffensive\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m report\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     66\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[1;32m     67\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args],\n\u001b[1;32m     68\u001b[0m                                  args[\u001b[38;5;241m-\u001b[39mextra_args:])]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1966\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   1861\u001b[0m \u001b[38;5;129m@_deprecate_positional_args\u001b[39m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassification_report\u001b[39m(y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, target_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1863\u001b[0m                           sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, digits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, output_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1864\u001b[0m                           zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1865\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \n\u001b[1;32m   1867\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;124;03m    <BLANKLINE>\u001b[39;00m\n\u001b[1;32m   1964\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1966\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1968\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1969\u001b[0m         labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:100\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# No metrics support \"multiclass-multioutput\" format\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    103\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n",
      "\u001b[0;31mValueError\u001b[0m: unknown is not supported"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "#your saved model name here\n",
    "model_name = './models/ht_bert_finetuned/' \n",
    "model = BERT_hate_speech.from_pretrained(model_name)\n",
    "\n",
    "# we don't batch our test set unless it's too big\n",
    "test_loader = DataLoader(test_dataset)\n",
    "\n",
    "report = evaluate(model, tokenizer, test_loader)\n",
    "\n",
    "print(report)\n",
    "\n",
    "print(report['accuracy'])\n",
    "print(report['Not offensive']['f1-score'])\n",
    "print(report['Offensive']['f1-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g9eJi1IaOGZ6",
   "metadata": {
    "id": "g9eJi1IaOGZ6"
   },
   "source": [
    "Let's test our model on a few sentences to get an intuition. Feel free to play around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49umxeBiK3Ec",
   "metadata": {
    "id": "49umxeBiK3Ec"
   },
   "outputs": [],
   "source": [
    "print(predict_hatespeech(\"I go see pinguins at the zoo.\", tokenizer, model))\n",
    "print(predict_hatespeech(\"Bananas are stupid\", tokenizer, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JaD0CmXMhEmA",
   "metadata": {
    "id": "JaD0CmXMhEmA"
   },
   "source": [
    "## Pre-training and finetuning BERT\n",
    "\n",
    "In this section, we will implement our own masked language modeling (MLM) training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KmT9ztohfiW3",
   "metadata": {
    "id": "KmT9ztohfiW3"
   },
   "source": [
    "### Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9-mAsIUYZ7Ex",
   "metadata": {
    "id": "9-mAsIUYZ7Ex"
   },
   "source": [
    "**Question 1: Add MLM head for pretraining**\n",
    "Your task is to fill in the following classes to implement MLM training: \n",
    "\n",
    "* ``PretrainDataset()``\n",
    "* ``Trainer_MLM()``\n",
    "* ``BERT_pretrain()``\n",
    "* ``main_pretrain()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C-3Jvmryw-ko",
   "metadata": {
    "id": "C-3Jvmryw-ko"
   },
   "source": [
    "To train our model in a MLM fashion, we need to make some adjustment to our ``Dataset`` class. We want to train BERT to predict an X% of tokens (in the original paper it is 15%) of which 80% will be replaced by a ``[MASK]`` token, 10% with a random token and 10% remain the same token.\n",
    "\n",
    "We introduce the function ``mask_tokens`` that will take care of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-ordinary",
   "metadata": {
    "id": "differential-ordinary"
   },
   "outputs": [],
   "source": [
    "class PretrainDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer, input_file):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.texts = self.read_text(input_file)\n",
    "\n",
    "        self.mlm_probability = 0.15\n",
    "        \n",
    "    def read_text(self, input_file):\n",
    "\n",
    "        with open(input_file, \"r\") as f:\n",
    "            self.texts = f.readlines()\n",
    "\n",
    "        \n",
    "    def collate_fn(self, batch):\n",
    "        batch = self.tokenizer(batch, return_tensors='pt', padding=True, \n",
    "                               truncation=True, max_length=128)\n",
    "        \n",
    "        inputs, labels = self.mask_tokens(batch[\"input_ids\"])\n",
    "        \n",
    "        return dict(input_ids=inputs, labels=labels)\n",
    "\n",
    "    \n",
    "    def mask_tokens(self, inputs):\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.\"\n",
    "            )\n",
    "        labels = inputs.clone()\n",
    "\n",
    "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "        ]\n",
    "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "        \n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
    "            probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-4yxeOG1Z94H",
   "metadata": {
    "id": "-4yxeOG1Z94H"
   },
   "source": [
    "The next step is to add a MLM head to our model. \n",
    "Use the ``BertOnlyMLMHead`` to add a MLM classifier to BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-fluid",
   "metadata": {
    "id": "beginning-fluid"
   },
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import BertOnlyMLMHead\n",
    "\n",
    "class BERT_pretrain(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        \n",
    "        self.bert =  BertModel()\n",
    "        self.cls = BertOnlyMLMHead()\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
    "                position_ids=None, head_mask=None, inputs_embeds=None, \n",
    "                labels=None, output_attentions=None, output_hidden_states=None,\n",
    "                return_dict=None):\n",
    "\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids, \n",
    "                            head_mask=head_mask,\n",
    "                            inputs_embeds=inputs_embeds,\n",
    "                            output_attentions=output_attentions,\n",
    "                            output_hidden_states=output_hidden_states,\n",
    "                            return_dict=return_dict)\n",
    "\n",
    "        ## Question 1 ##\n",
    "        prediction_scores =  self.cls(outputs[0])\n",
    "\n",
    "        return prediction_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46_QOzQfLgME",
   "metadata": {
    "id": "46_QOzQfLgME"
   },
   "source": [
    "We will define a new Trainer class for pre-training. \n",
    "\n",
    "**Note:** We could use the standard ``Trainer`` class to train our model. Then we would need to make ``BERT_pretrain`` output  ``loss`` and BERT ``outputs`` as a tuple``(loss, outputs)``.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xoG-AvUdzyqt",
   "metadata": {
    "id": "xoG-AvUdzyqt"
   },
   "outputs": [],
   "source": [
    "class Trainer_MLM(Trainer):\n",
    "    def compute_loss(self, model, inputs):\n",
    "        \n",
    "        labels = inputs['labels']\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # MLM loss\n",
    "        lm_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        loss_mlm = lm_loss(outputs.view(-1, model.config.vocab_size), labels.view(-1))\n",
    "        \n",
    "        loss = loss_mlm\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q3xGoHTSh-Yu",
   "metadata": {
    "id": "q3xGoHTSh-Yu"
   },
   "source": [
    "Finally, put everything together in the ``main_pretrain()`` class. \n",
    "\n",
    "In the code below, write code to pre-train your custom MLM model on ``pretrain.txt`` file found in the ``data`` folder.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-singles",
   "metadata": {
    "id": "familiar-singles"
   },
   "outputs": [],
   "source": [
    "def main_pretrain():\n",
    "    \n",
    "    ## Question 1 ##\n",
    "\n",
    "    model = BERT_pretrain.from_pretrained(\"bert-base-case\")  \n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-case\")\n",
    "\n",
    "    train_args = TrainingArguments(output_dir='./experiment/pretrain',\n",
    "                                   learning_rate = 0.00005,\n",
    "                                   num_train_epochs =1,\n",
    "                                   save_steps = 10000,\n",
    "                                   per_device_train_batch_size=64)\n",
    "    \n",
    "    trainer = Trainer_MLM(model=model, args=train_args,\n",
    "                          train_dataset=pretrain_dataset,\n",
    "                          data_collator=pretrain_dataset.collate_fn)\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model('./models/ht_bert_pretrained/')\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4-tsbImSJvyf",
   "metadata": {
    "id": "4-tsbImSJvyf"
   },
   "source": [
    "Running the pretraining will take ~ 2 hours with one epoch and batchsize 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-signal",
   "metadata": {
    "id": "greenhouse-signal"
   },
   "outputs": [],
   "source": [
    " main_pretrain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-sleep",
   "metadata": {
    "id": "incorporated-sleep"
   },
   "source": [
    "### Finetuning\n",
    "\n",
    "**Question 2: Load the pretrained model for finetuning**\n",
    "\n",
    "In the code below modify the ``main_hate_speech`` function from earlier to import the model we just trained, and finetune it on our ``OlidDataset`` train sets.\n",
    "\n",
    "**Note**: Your pre-trained model is saved as checkpoint files in your ``output_dir`` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-cliff",
   "metadata": {
    "id": "metropolitan-cliff"
   },
   "outputs": [],
   "source": [
    "def main_hate_speech():\n",
    "\n",
    "    #call our custom BERT model and pass as parameter the name of an available pretrained model\n",
    "    model_name = './models/ht_bert_pretrained/'\n",
    "    model = BERT_hate_speech.from_pretrained(model_name)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./experiment/hate_speech',\n",
    "        learning_rate = 0.0001,\n",
    "        logging_steps= 100,\n",
    "        per_device_train_batch_size=32,\n",
    "        num_train_epochs = 3,\n",
    "    )\n",
    "    trainer = Trainer_hate_speech(\n",
    "        model=model,                         \n",
    "        args=training_args,                 \n",
    "        train_dataset=train_dataset,                   \n",
    "        data_collator=train_dataset.collate_fn\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.save_model('./models/ht_bert_finetuned/')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-exemption",
   "metadata": {
    "id": "simplified-exemption"
   },
   "outputs": [],
   "source": [
    "main_hate_speech()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eqtu-ECSaUO",
   "metadata": {
    "id": "8eqtu-ECSaUO"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qCC3iV37M7np",
   "metadata": {
    "id": "qCC3iV37M7np"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "#your saved model name here\n",
    "model_name = './models/ht_bert_pretrained_finetuned/' \n",
    "model = BERT_hate_speech.from_pretrained(model_name)\n",
    "\n",
    "test_loader = DataLoader(test_dataset)\n",
    "\n",
    "report = evaluate(model, tokenizer, test_loader)\n",
    "\n",
    "print(report)\n",
    "print(report['accuracy'])\n",
    "print(report['Not offensive']['f1-score'])\n",
    "print(report['Offensive']['f1-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-courtesy",
   "metadata": {
    "id": "existing-courtesy"
   },
   "source": [
    "## Multi-task Hate Speech Classification\n",
    "\n",
    "It's time to add the two other tasks to our implementation of ``BERT_hate_speech()``.\n",
    "\n",
    "**Question 3: Add multi-heads (task b, task c) for multi-task hatespeech classification**\n",
    "\n",
    "Fill in the missing code from the following classes:\n",
    "\n",
    "* ``BERT_hate_speech_multitask()``\n",
    "* `` Trainer_hate_speech_multitask()``\n",
    "* ``main_hate_speech_multitask()``\n",
    "* ``predict_hatespeech_multitask()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ogHc8w4Kyew",
   "metadata": {
    "id": "9ogHc8w4Kyew"
   },
   "source": [
    "### Multi-task Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-teddy",
   "metadata": {
    "id": "personalized-teddy"
   },
   "outputs": [],
   "source": [
    "class BERT_hate_speech_multitask(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # BERT Model\n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        # Task A\n",
    "        self.projection_a = torch.nn.Sequential(torch.nn.Dropout(0.2),\n",
    "                                                torch.nn.Linear(config.hidden_size, 2))\n",
    "        \n",
    "        ##  Question 3 ##\n",
    "\n",
    "        # Task B\n",
    "        self.projection_b =  # <TODO> #\n",
    "\n",
    "        # Task C\n",
    "        self.projection_c = # <TODO> #\n",
    "        \n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # Task A\n",
    "        logits_a = self.projection_a(outputs[1])\n",
    "        \n",
    "        ##  Question 3 ##\n",
    "\n",
    "        # Task B\n",
    "        # <TODO> #\n",
    "      \n",
    "        # Task C \n",
    "        # <TODO> #\n",
    "\n",
    "        return (logits_a, logits_b, logits_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lwCad-uo1pnP",
   "metadata": {
    "id": "lwCad-uo1pnP"
   },
   "outputs": [],
   "source": [
    "class Trainer_hate_speech_multitask(Trainer):\n",
    "    def compute_loss(self, model, inputs):\n",
    "        labels = {}\n",
    "        labels['label_a'] = inputs.pop('label_a')\n",
    "        labels['label_b'] = inputs.pop('label_b')\n",
    "        labels['label_c'] = inputs.pop('label_c')\n",
    "\n",
    "        (out_a, out_b, out_c) = model(**inputs)\n",
    "\n",
    "        # LOSS A\n",
    "        loss_task_a = nn.CrossEntropyLoss()\n",
    "        labels_a = labels['label_a']\n",
    "        loss_a = loss_task_a(out_a.view(-1, 2), labels_a.view(-1))\n",
    "\n",
    "        ## QUESTION 3 ##        \n",
    "        # LOSS B\n",
    "        # <TODO> #\n",
    "\n",
    "        # LOSS C\n",
    "        # <TODO> #\n",
    "\n",
    "        loss = loss_a + loss_b + loss_c\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LSA-HdbWjpAl",
   "metadata": {
    "id": "LSA-HdbWjpAl"
   },
   "source": [
    "Just as in the finetuning task, instantiate a ``BERT_hate_speech_multitask`` model from an pre-trained model and finetune it on our ``train_dataset``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-short",
   "metadata": {
    "id": "satisfied-short"
   },
   "outputs": [],
   "source": [
    "def main_hate_speech_multitask():\n",
    "    ##  Question 3 ##\n",
    "\n",
    "    # <TODO> #\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-P1ekYHaqPwR",
   "metadata": {
    "id": "-P1ekYHaqPwR"
   },
   "source": [
    "Running the code below should take ~10 min for 3 epochs and batch size 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-trading",
   "metadata": {
    "id": "wrapped-trading"
   },
   "outputs": [],
   "source": [
    "main_hate_speech_multitask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p2EFyb3rKtVB",
   "metadata": {
    "id": "p2EFyb3rKtVB"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8EtaoYYGJ6J2",
   "metadata": {
    "id": "8EtaoYYGJ6J2"
   },
   "outputs": [],
   "source": [
    "def predict_hatespeech_multitask(input, tokenizer, model): \n",
    "  model.eval()\n",
    "  encodings = tokenizer(input, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "  \n",
    "  (out1, out2, out3) = model(**encodings)\n",
    "  \n",
    "  #task a\n",
    "  preds_a = torch.max(out1, 1)\n",
    "\n",
    "  # task b\n",
    "  # <TODO> #\n",
    "  \n",
    "  # task c\n",
    "  # <TODO> #\n",
    "\n",
    "  preds = (preds_a[1], preds_b[1], preds_c[1])\n",
    "  scores = (preds_a[0], preds_b[0], preds_c[0])\n",
    "\n",
    "  return {'predictions':preds, 'confidences':scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z5pIqvWoLbDy",
   "metadata": {
    "id": "z5pIqvWoLbDy"
   },
   "outputs": [],
   "source": [
    "def evaluate_multitask(model, tokenizer, data_loader):\n",
    "\n",
    "  task_num = 3\n",
    "  total_count = 0\n",
    "  correct_count = [0] * task_num  \n",
    "  accuracies = [0] * task_num\n",
    "\n",
    "  batch_size = test_loader.batch_size\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for data in tqdm(data_loader): \n",
    "\n",
    "      labels = {}\n",
    "      labels['label_a'] = data['label_a']\n",
    "      labels['label_b'] = data['label_b']\n",
    "      labels['label_c'] = data['label_c']\n",
    "\n",
    "      tweets = data['text']\n",
    "\n",
    "      pred = predict_hatespeech_multitask(tweets, tokenizer, model)\n",
    "\n",
    "      preds = pred['predictions'] \n",
    "\n",
    "      for i, label in enumerate(labels):\n",
    "        correct_count[i]+= torch.mean((preds[i] == labels[label]).float())\n",
    "\n",
    "      total_count += np.float(batch_size)\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "      accuracies[i] = (correct_count[i]/total_count)\n",
    "\n",
    " \n",
    "  return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P24QVnAULiMd",
   "metadata": {
    "id": "P24QVnAULiMd"
   },
   "outputs": [],
   "source": [
    "model = BERT_hate_speech_multitask.from_pretrained(\"./models/ht_bert_multi_finetuned/\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "test_loader = DataLoader(test_dataset)\n",
    "\n",
    "accuracies = evaluate_multitask(model, tokenizer, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-h8pw9i8L8NJ",
   "metadata": {
    "id": "-h8pw9i8L8NJ"
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print('Task %d accuracy: %2.2f %%' % (i, 100.0*accuracies[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yA_vw43f-UX4",
   "metadata": {
    "id": "yA_vw43f-UX4"
   },
   "outputs": [],
   "source": [
    "print(predict_hatespeech_multitask(\"I go see pinguins at the zoo.\", tokenizer, model)['predictions'])\n",
    "print(predict_hatespeech_multitask(\"Bananas are so stupid \", tokenizer, model)['predictions'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab06.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0712735baf9b40dea3e3b38f00cb6f1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91971772860941ac8ab83cde9292fc3a",
      "max": 1132,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_968b7b1d37064914a5c00b00f500d9e2",
      "value": 1132
     }
    },
    "20fb0725d0e34e1d9c7a67d38b9bda2d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25e987dead5e40cbafb71feb099cda81": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36f7441e69d24a359d26ebf9fa4af07f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3b078a8aa9b145dabf05a8ca93854fc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0712735baf9b40dea3e3b38f00cb6f1e",
       "IPY_MODEL_775b4ca6c0e94ba08c4ef32779311afa"
      ],
      "layout": "IPY_MODEL_25e987dead5e40cbafb71feb099cda81"
     }
    },
    "58a5b881825b461e8d8c220d69f17734": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "70f56f9605cc4ef086164912440686d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "775b4ca6c0e94ba08c4ef32779311afa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6e9683c361240ec85d67aa39775b709",
      "placeholder": "​",
      "style": "IPY_MODEL_8a9c46189a7449939de1023699ce1ed0",
      "value": " 1.13k/1.13k [00:12&lt;00:00, 89.4B/s]"
     }
    },
    "798595cb96314ae7bc6e502c788f94b7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7aaf330836db446ea1bef05f34884cb4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f51eba2be474e5486ca9a3dc1487037": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_adba8794c6d6470083d805d42111b4d6",
       "IPY_MODEL_c0d35b06063c4113aa88d899fc26a28a"
      ],
      "layout": "IPY_MODEL_a6b2c87fafa04f3fa49c11cf47a0e527"
     }
    },
    "7f87a9f9fda245bab41de3aa49ac4c62": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a9c46189a7449939de1023699ce1ed0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "91971772860941ac8ab83cde9292fc3a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "968b7b1d37064914a5c00b00f500d9e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "980bf5380ccc467284f68a701985951e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6b2c87fafa04f3fa49c11cf47a0e527": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6e9683c361240ec85d67aa39775b709": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "adba8794c6d6470083d805d42111b4d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f87a9f9fda245bab41de3aa49ac4c62",
      "max": 297928209,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_36f7441e69d24a359d26ebf9fa4af07f",
      "value": 297928209
     }
    },
    "c0d35b06063c4113aa88d899fc26a28a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f81ac79ac55e45c7908113bf8a69c365",
      "placeholder": "​",
      "style": "IPY_MODEL_980bf5380ccc467284f68a701985951e",
      "value": " 298M/298M [00:05&lt;00:00, 50.3MB/s]"
     }
    },
    "d90b02be82ab42eca0908ed9a2c83f08": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e0354eb9172240539234a2559376ff8e",
       "IPY_MODEL_eab6cd58869e452a8704cffcf1234a36"
      ],
      "layout": "IPY_MODEL_798595cb96314ae7bc6e502c788f94b7"
     }
    },
    "e0354eb9172240539234a2559376ff8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "  9%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7aaf330836db446ea1bef05f34884cb4",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_70f56f9605cc4ef086164912440686d2",
      "value": 93
     }
    },
    "eab6cd58869e452a8704cffcf1234a36": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20fb0725d0e34e1d9c7a67d38b9bda2d",
      "placeholder": "​",
      "style": "IPY_MODEL_58a5b881825b461e8d8c220d69f17734",
      "value": " 93/1000 [01:28&lt;15:29,  1.03s/it]"
     }
    },
    "f81ac79ac55e45c7908113bf8a69c365": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
